{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning it is quite usual to have to deal with imbalanced dataset. This is particularly true in online learning for tasks such as fraud detection and spam classification. In these two cases, which are binary classification problems, there are usually many more 0s than 1s, which generally hinders the performance of the classifiers we thrown at them.\n",
    "\n",
    "As an example we'll use the credit card dataset available in `creme`. We'll first use a `collections.Counter` to count the number of 0s and 1s in order to get an idea of the class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 284315 (99.82725%)\n",
      "1: 492 (0.17275%)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from creme import datasets\n",
    "\n",
    "X_y = datasets.CreditCard()\n",
    "\n",
    "counts = collections.Counter(y for _, y in X_y)\n",
    "\n",
    "for c, count in counts.items():\n",
    "    print(f'{c}: {count} ({count / sum(counts.values()):.5%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is quite unbalanced. For each 1 there are about 578 0s. Let's now train a logistic regression with default parameters and see how well it does. We'll measure the ROC AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15,000] ROCAUC: 0.899341 – 0:00:03\n",
      "[30,000] ROCAUC: 0.87079 – 0:00:06\n",
      "[45,000] ROCAUC: 0.899804 – 0:00:10\n",
      "[60,000] ROCAUC: 0.89192 – 0:00:13\n",
      "[75,000] ROCAUC: 0.890126 – 0:00:16\n",
      "[90,000] ROCAUC: 0.897645 – 0:00:20\n",
      "[105,000] ROCAUC: 0.889682 – 0:00:23\n",
      "[120,000] ROCAUC: 0.886271 – 0:00:26\n",
      "[135,000] ROCAUC: 0.883233 – 0:00:30\n",
      "[150,000] ROCAUC: 0.885329 – 0:00:35\n",
      "[165,000] ROCAUC: 0.897751 – 0:00:38\n",
      "[180,000] ROCAUC: 0.896706 – 0:00:42\n",
      "[195,000] ROCAUC: 0.896068 – 0:00:45\n",
      "[210,000] ROCAUC: 0.894425 – 0:00:48\n",
      "[225,000] ROCAUC: 0.893745 – 0:00:52\n",
      "[240,000] ROCAUC: 0.893375 – 0:00:55\n",
      "[255,000] ROCAUC: 0.89189 – 0:00:58\n",
      "[270,000] ROCAUC: 0.893778 – 0:01:04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.891071"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from creme import linear_model\n",
    "from creme import metrics\n",
    "from creme import model_selection\n",
    "from creme import preprocessing\n",
    "\n",
    "\n",
    "X_y = datasets.CreditCard()\n",
    "\n",
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    linear_model.LogisticRegression()\n",
    ")\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "model_selection.progressive_val_score(\n",
    "    X_y,\n",
    "    model,\n",
    "    metric,\n",
    "    print_every=15_000,\n",
    "    show_time=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is already quite acceptable, but as we will now see we can do even better. The first thing we can do is to add weight to the 1s by using the `weight_pos` argument of the `Log` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15,000] ROCAUC: 0.906964 – 0:00:03\n",
      "[30,000] ROCAUC: 0.918349 – 0:00:06\n",
      "[45,000] ROCAUC: 0.938641 – 0:00:10\n",
      "[60,000] ROCAUC: 0.925495 – 0:00:14\n",
      "[75,000] ROCAUC: 0.920024 – 0:00:18\n",
      "[90,000] ROCAUC: 0.925929 – 0:00:22\n",
      "[105,000] ROCAUC: 0.915362 – 0:00:25\n",
      "[120,000] ROCAUC: 0.912398 – 0:00:28\n",
      "[135,000] ROCAUC: 0.911679 – 0:00:32\n",
      "[150,000] ROCAUC: 0.910731 – 0:00:35\n",
      "[165,000] ROCAUC: 0.91989 – 0:00:39\n",
      "[180,000] ROCAUC: 0.919865 – 0:00:43\n",
      "[195,000] ROCAUC: 0.918189 – 0:00:46\n",
      "[210,000] ROCAUC: 0.917085 – 0:00:50\n",
      "[225,000] ROCAUC: 0.916455 – 0:00:53\n",
      "[240,000] ROCAUC: 0.917223 – 0:00:57\n",
      "[255,000] ROCAUC: 0.916282 – 0:01:00\n",
      "[270,000] ROCAUC: 0.917515 – 0:01:03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.914269"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from creme import optim\n",
    "\n",
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    linear_model.LogisticRegression(\n",
    "        loss=optim.losses.Log(weight_pos=5)\n",
    "    )\n",
    ")\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "model_selection.progressive_val_score(\n",
    "    X_y,\n",
    "    model,\n",
    "    metric,\n",
    "    print_every=15_000,\n",
    "    show_time=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focal loss\n",
    "\n",
    "Deep learning for object detection community has produced a special loss function for imbalaced learning called [focal loss](https://arxiv.org/pdf/1708.02002.pdf). We are doing binary classification, so we can plug the binary version of focal loss into our logistic regression and see how well it fairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15,000] ROCAUC: 0.907819 – 0:00:03\n",
      "[30,000] ROCAUC: 0.902699 – 0:00:06\n",
      "[45,000] ROCAUC: 0.926 – 0:00:10\n",
      "[60,000] ROCAUC: 0.914868 – 0:00:13\n",
      "[75,000] ROCAUC: 0.910841 – 0:00:17\n",
      "[90,000] ROCAUC: 0.920638 – 0:00:20\n",
      "[105,000] ROCAUC: 0.914711 – 0:00:23\n",
      "[120,000] ROCAUC: 0.911864 – 0:00:27\n",
      "[135,000] ROCAUC: 0.911162 – 0:00:30\n",
      "[150,000] ROCAUC: 0.910189 – 0:00:33\n",
      "[165,000] ROCAUC: 0.918094 – 0:00:37\n",
      "[180,000] ROCAUC: 0.916794 – 0:00:40\n",
      "[195,000] ROCAUC: 0.915302 – 0:00:44\n",
      "[210,000] ROCAUC: 0.913043 – 0:00:47\n",
      "[225,000] ROCAUC: 0.912626 – 0:00:50\n",
      "[240,000] ROCAUC: 0.914699 – 0:00:54\n",
      "[255,000] ROCAUC: 0.914963 – 0:00:57\n",
      "[270,000] ROCAUC: 0.916283 – 0:01:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.913071"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    linear_model.LogisticRegression(loss=optim.losses.BinaryFocalLoss(2, 1))\n",
    ")\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "model_selection.progressive_val_score(\n",
    "    X_y,\n",
    "    model,\n",
    "    metric,\n",
    "    print_every=15_000,\n",
    "    show_time=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under-sampling the majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding importance weights only works with gradient-based models (which includes neural networks). A more generic, and potentially more effective approach, is to use undersamplig and oversampling. As an example, we'll under-sample the stream so that our logistic regression encounter 20% of 1s and 80% of 0s. Under-sampling has the additional benefit of requiring less training steps, and thus reduces the total training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15,000] ROCAUC: 0.939514 – 0:00:02\n",
      "[30,000] ROCAUC: 0.948435 – 0:00:05\n",
      "[45,000] ROCAUC: 0.965093 – 0:00:08\n",
      "[60,000] ROCAUC: 0.956687 – 0:00:11\n",
      "[75,000] ROCAUC: 0.950131 – 0:00:14\n",
      "[90,000] ROCAUC: 0.956866 – 0:00:17\n",
      "[105,000] ROCAUC: 0.947408 – 0:00:20\n",
      "[120,000] ROCAUC: 0.942359 – 0:00:22\n",
      "[135,000] ROCAUC: 0.941693 – 0:00:25\n",
      "[150,000] ROCAUC: 0.943845 – 0:00:28\n",
      "[165,000] ROCAUC: 0.949408 – 0:00:31\n",
      "[180,000] ROCAUC: 0.948932 – 0:00:34\n",
      "[195,000] ROCAUC: 0.948422 – 0:00:37\n",
      "[210,000] ROCAUC: 0.948689 – 0:00:40\n",
      "[225,000] ROCAUC: 0.947371 – 0:00:42\n",
      "[240,000] ROCAUC: 0.949912 – 0:00:45\n",
      "[255,000] ROCAUC: 0.949748 – 0:00:48\n",
      "[270,000] ROCAUC: 0.950999 – 0:00:51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.948824"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from creme import imblearn\n",
    "\n",
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    imblearn.RandomUnderSampler(\n",
    "        classifier=linear_model.LogisticRegression(),\n",
    "        desired_dist={0: .8, 1: .2},\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "model_selection.progressive_val_score(\n",
    "    X_y,\n",
    "    model,\n",
    "    metric,\n",
    "    print_every=15_000,\n",
    "    show_time=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RandomUnderSampler` class is a wrapper for classifiers. This is represented by a rectangle around the logistic regression bubble when we draw the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"167pt\" height=\"263pt\"\n",
       " viewBox=\"0.00 0.00 167.00 263.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 259)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-259 163,-259 163,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_RandomUnderSampler</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"8,-64 8,-139 151,-139 151,-64 8,-64\"/>\n",
       "<text text-anchor=\"middle\" x=\"79.5\" y=\"-123.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RandomUnderSampler</text>\n",
       "</g>\n",
       "<!-- x -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" stroke-width=\"1.2\" points=\"106,-255 52,-255 52,-219 106,-219 106,-255\"/>\n",
       "<text text-anchor=\"middle\" x=\"79\" y=\"-234.2\" font-family=\"trebuchet\" font-size=\"11.00\" fill=\"#000000\">x</text>\n",
       "</g>\n",
       "<!-- StandardScaler -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>StandardScaler</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" stroke-width=\"1.2\" points=\"123,-183 35,-183 35,-147 123,-147 123,-183\"/>\n",
       "<text text-anchor=\"middle\" x=\"79\" y=\"-162.2\" font-family=\"trebuchet\" font-size=\"11.00\" fill=\"#000000\">StandardScaler</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;StandardScaler -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x&#45;&gt;StandardScaler</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-width=\".6\" d=\"M79,-218.8314C79,-218.8314 79,-193.4133 79,-193.4133\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" stroke-width=\".6\" points=\"82.5001,-193.4132 79,-183.4133 75.5001,-193.4133 82.5001,-193.4132\"/>\n",
       "</g>\n",
       "<!-- LogisticRegression -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>LogisticRegression</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" stroke-width=\"1.2\" points=\"131.5,-108 26.5,-108 26.5,-72 131.5,-72 131.5,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"79\" y=\"-87.2\" font-family=\"trebuchet\" font-size=\"11.00\" fill=\"#000000\">LogisticRegression</text>\n",
       "</g>\n",
       "<!-- StandardScaler&#45;&gt;LogisticRegression -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>StandardScaler&#45;&gt;LogisticRegression</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-width=\".6\" d=\"M79,-146.8446C79,-146.8446 79,-118.2482 79,-118.2482\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" stroke-width=\".6\" points=\"82.5001,-118.2481 79,-108.2482 75.5001,-118.2482 82.5001,-118.2481\"/>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>y</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" stroke-width=\"1.2\" points=\"106,-36 52,-36 52,0 106,0 106,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"79\" y=\"-15.2\" font-family=\"trebuchet\" font-size=\"11.00\" fill=\"#000000\">y</text>\n",
       "</g>\n",
       "<!-- LogisticRegression&#45;&gt;y -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>LogisticRegression&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-width=\".6\" d=\"M79,-71.8314C79,-71.8314 79,-46.4133 79,-46.4133\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" stroke-width=\".6\" points=\"82.5001,-46.4132 79,-36.4133 75.5001,-46.4133 82.5001,-46.4132\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fb07d7dfc50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-sampling the minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also attain the same class distribution by over-sampling the minority class. This will come at cost of having to train with more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15,000] ROCAUC: 0.939001 – 0:00:03\n",
      "[30,000] ROCAUC: 0.928518 – 0:00:07\n",
      "[45,000] ROCAUC: 0.948975 – 0:00:10\n",
      "[60,000] ROCAUC: 0.936699 – 0:00:14\n",
      "[75,000] ROCAUC: 0.9297 – 0:00:17\n",
      "[90,000] ROCAUC: 0.934275 – 0:00:21\n",
      "[105,000] ROCAUC: 0.924655 – 0:00:24\n",
      "[120,000] ROCAUC: 0.91896 – 0:00:27\n",
      "[135,000] ROCAUC: 0.917809 – 0:00:31\n",
      "[150,000] ROCAUC: 0.916056 – 0:00:34\n",
      "[165,000] ROCAUC: 0.925535 – 0:00:38\n",
      "[180,000] ROCAUC: 0.925481 – 0:00:41\n",
      "[195,000] ROCAUC: 0.923488 – 0:00:45\n",
      "[210,000] ROCAUC: 0.922177 – 0:00:48\n",
      "[225,000] ROCAUC: 0.921203 – 0:00:52\n",
      "[240,000] ROCAUC: 0.920579 – 0:00:55\n",
      "[255,000] ROCAUC: 0.920353 – 0:00:59\n",
      "[270,000] ROCAUC: 0.921487 – 0:01:02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.918082"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    imblearn.RandomOverSampler(\n",
    "        classifier=linear_model.LogisticRegression(),\n",
    "        desired_dist={0: .8, 1: .2},\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "model_selection.progressive_val_score(\n",
    "    X_y,\n",
    "    model,\n",
    "    metric,\n",
    "    print_every=15_000,\n",
    "    show_time=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with a desired sample size\n",
    "\n",
    "The downside of both `RandomUnderSampler` and `RandomOverSampler` is that you don't have any control on the amount of data the classifier trains on. The number of samples is adjusted so that the target distribution can be attained, either by under-sampling or over-sampling. However, you can do both at the same time and choose how much data the classifier will see. To do so, we can use the `RandomSampler` class. In addition to the desired class distribution, we can specify how much data to train on. The samples will both be under-sampled and over-sampled in order to fit your constraints. This is powerful because it allows you to control both the class distribution and the size of the training data (and thus the training time). In the following example we'll set it so that the model will train with 1 percent of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15,000] ROCAUC: 0.946234 – 0:00:02\n",
      "[30,000] ROCAUC: 0.956145 – 0:00:05\n",
      "[45,000] ROCAUC: 0.972701 – 0:00:08\n",
      "[60,000] ROCAUC: 0.955377 – 0:00:11\n",
      "[75,000] ROCAUC: 0.949038 – 0:00:14\n",
      "[90,000] ROCAUC: 0.959151 – 0:00:17\n",
      "[105,000] ROCAUC: 0.946733 – 0:00:19\n",
      "[120,000] ROCAUC: 0.943587 – 0:00:22\n",
      "[135,000] ROCAUC: 0.944251 – 0:00:25\n",
      "[150,000] ROCAUC: 0.946132 – 0:00:28\n",
      "[165,000] ROCAUC: 0.951248 – 0:00:31\n",
      "[180,000] ROCAUC: 0.95303 – 0:00:34\n",
      "[195,000] ROCAUC: 0.95088 – 0:00:37\n",
      "[210,000] ROCAUC: 0.951606 – 0:00:39\n",
      "[225,000] ROCAUC: 0.949907 – 0:00:42\n",
      "[240,000] ROCAUC: 0.95294 – 0:00:45\n",
      "[255,000] ROCAUC: 0.952255 – 0:00:48\n",
      "[270,000] ROCAUC: 0.953966 – 0:00:51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.951294"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    imblearn.RandomSampler(\n",
    "        classifier=linear_model.LogisticRegression(),\n",
    "        desired_dist={0: .8, 1: .2},\n",
    "        sampling_rate=.01,\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "model_selection.progressive_val_score(\n",
    "    X_y,\n",
    "    model,\n",
    "    metric,\n",
    "    print_every=15_000,\n",
    "    show_time=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid approach\n",
    "\n",
    "As you might have guessed by now, nothing is stopping you from mixing imbalanced learning methods together. As an example, let's combine `imblearn.RandomUnderSampler` and the `weight_pos` parameter from the `optim.losses.Log` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15,000] ROCAUC: 0.957494 – 0:00:02\n",
      "[30,000] ROCAUC: 0.967154 – 0:00:05\n",
      "[45,000] ROCAUC: 0.976699 – 0:00:08\n",
      "[60,000] ROCAUC: 0.974157 – 0:00:11\n",
      "[75,000] ROCAUC: 0.970901 – 0:00:14\n",
      "[90,000] ROCAUC: 0.974191 – 0:00:17\n",
      "[105,000] ROCAUC: 0.964133 – 0:00:19\n",
      "[120,000] ROCAUC: 0.963783 – 0:00:22\n",
      "[135,000] ROCAUC: 0.964123 – 0:00:25\n",
      "[150,000] ROCAUC: 0.965729 – 0:00:28\n",
      "[165,000] ROCAUC: 0.968144 – 0:00:31\n",
      "[180,000] ROCAUC: 0.968855 – 0:00:34\n",
      "[195,000] ROCAUC: 0.968339 – 0:00:37\n",
      "[210,000] ROCAUC: 0.967406 – 0:00:39\n",
      "[225,000] ROCAUC: 0.966696 – 0:00:42\n",
      "[240,000] ROCAUC: 0.968059 – 0:00:45\n",
      "[255,000] ROCAUC: 0.968884 – 0:00:48\n",
      "[270,000] ROCAUC: 0.969511 – 0:00:51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.968294"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    imblearn.RandomUnderSampler(\n",
    "        classifier=linear_model.LogisticRegression(\n",
    "            loss=optim.losses.Log(weight_pos=5)\n",
    "        ),\n",
    "        desired_dist={0: .8, 1: .2},\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "model_selection.progressive_val_score(\n",
    "    X_y,\n",
    "    model,\n",
    "    metric,\n",
    "    print_every=15_000,\n",
    "    show_time=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
