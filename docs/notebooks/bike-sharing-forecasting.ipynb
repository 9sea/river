{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike-sharing forecasting (regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we're going to forecast the number of bikes in 5 bike stations from the city of Toulouse. We'll do so by building a simple model step by step. The dataset contains 182,470 observations. Let's first take a peak at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clouds': 75,\n",
      " 'description': 'light rain',\n",
      " 'humidity': 81,\n",
      " 'moment': datetime.datetime(2016, 4, 1, 0, 0, 7),\n",
      " 'pressure': 1017.0,\n",
      " 'station': 'metro-canal-du-midi',\n",
      " 'temperature': 6.54,\n",
      " 'wind': 9.3}\n",
      "Number of available bikes: 1\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from creme import datasets\n",
    "\n",
    "X_y = datasets.Bikes()\n",
    "\n",
    "for x, y in X_y:\n",
    "    pprint(x)\n",
    "    print(f'Number of available bikes: {y}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by using a simple linear regression on the numeric features. We can select the numeric features and discard the rest of the features using a `Whitelister`. Linear regression is very likely to go haywire if we don't scale the data, so we'll use a `StandardScaler` to do just that. We'll evaluate the model by measuring the mean absolute error. Finally we'll print the score every 20,000 observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 0."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from creme import compose\n",
    "from creme import linear_model\n",
    "from creme import metrics\n",
    "from creme import model_selection\n",
    "from creme import preprocessing\n",
    "\n",
    "X_y = datasets.Bikes()\n",
    "\n",
    "model = compose.Whitelister('clouds', 'humidity', 'pressure', 'temperature', 'wind')\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "metric = metrics.MAE()\n",
    "\n",
    "model_selection.progressive_val_score(X_y, model, metric, print_every=20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesn't seem to be doing that well, but then again we didn't provide with a lot of features. A generally good idea for this kind of problem is to look at an average of the previous values. For example, for each station we can look at the average number of bikes per hour. To do so we first have to extract the hour from the  `moment` field. We can then use a `TargetAgg` to aggregate the values of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 0."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from creme import feature_extraction\n",
    "from creme import stats\n",
    "\n",
    "X_y = iter(datasets.Bikes())\n",
    "\n",
    "def get_hour(x):\n",
    "    x['hour'] = x['moment'].hour\n",
    "    return x\n",
    "\n",
    "model = compose.Whitelister('clouds', 'humidity', 'pressure', 'temperature', 'wind')\n",
    "model += (\n",
    "    get_hour |\n",
    "    feature_extraction.TargetAgg(by=['station', 'hour'], how=stats.Mean())\n",
    ")\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "metric = metrics.MAE()\n",
    "\n",
    "model_selection.progressive_val_score(X_y, model, metric, print_every=20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding a single feature, we've managed to significantly reduce the mean absolute error. At this point you might think that the model is getting slightly complex, and is difficult to understand and test. Pipelines have the advantage of being terse, but they aren't always to debug. Thankfully `creme` has some ways to relieve the pain.\n",
    "\n",
    "The first thing we can do it to draw the pipeline, to get an idea of how the data flows through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"530pt\" height=\"404pt\"\n",
       " viewBox=\"0.00 0.00 530.00 404.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 400)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-400 526,-400 526,4 -4,4\"/>\n",
       "<!-- x -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" stroke-width=\"1.2\" points=\"314.5,-396 260.5,-396 260.5,-360 314.5,-360 314.5,-396\"/>\n",
       "<text text-anchor=\"middle\" x=\"287.5\" y=\"-375.2\" font-family=\"trebuchet\" font-size=\"11.00\" fill=\"#000000\">x</text>\n",
       "</g>\n",
       "<!-- (&#39;clouds&#39;, &#39;humidity&#39;, &#39;pressure&#39;, &#39;temperature&#39;, &#39;wind&#39;) -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>(&#39;clouds&#39;, &#39;humidity&#39;, &#39;pressure&#39;, &#39;temperature&#39;, &#39;wind&#39;)</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" stroke-width=\"1.2\" points=\"305,-252 0,-252 0,-216 305,-216 305,-252\"/>\n",
       "<text text-anchor=\"middle\" x=\"152.5\" y=\"-231.2\" font-family=\"trebuchet\" font-size=\"11.00\" fill=\"#000000\">(&#39;clouds&#39;, &#39;humidity&#39;, &#39;pressure&#39;, &#39;temperature&#39;, &#39;wind&#39;)</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;(&#39;clouds&#39;, &#39;humidity&#39;, &#39;pressure&#39;, &#39;temperature&#39;, &#39;wind&#39;) -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x&#45;&gt;(&#39;clouds&#39;, &#39;humidity&#39;, &#39;pressure&#39;, &#39;temperature&#39;, &#39;wind&#39;)</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-width=\".6\" d=\"M282.75,-359.7623C282.75,-359.7623 282.75,-262.0896 282.75,-262.0896\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" stroke-width=\".6\" points=\"286.2501,-262.0896 282.75,-252.0896 279.2501,-262.0897 286.2501,-262.0896\"/>\n",
       "</g>\n",
       "<!-- get_hour -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>get_hour</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" stroke-width=\"1.2\" points=\"395.5,-324 341.5,-324 341.5,-288 395.5,-288 395.5,-324\"/>\n",
       "<text text-anchor=\"middle\" x=\"368.5\" y=\"-303.2\" font-family=\"trebuchet\" font-size=\"11.00\" fill=\"#000000\">get_hour</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;get_hour -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>x&#45;&gt;get_hour</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-width=\".6\" d=\"M309.75,-359.8314C309.75,-338.502 309.75,-306 309.75,-306 309.75,-306 331.1889,-306 331.1889,-306\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" stroke-width=\".6\" points=\"331.189,-309.5001 341.1889,-306 331.1889,-302.5001 331.189,-309.5001\"/>\n",
       "</g>\n",
       "<!-- StandardScaler -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>StandardScaler</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" stroke-width=\"1.2\" points=\"331.5,-180 243.5,-180 243.5,-144 331.5,-144 331.5,-180\"/>\n",
       "<text text-anchor=\"middle\" x=\"287.5\" y=\"-159.2\" font-family=\"trebuchet\" font-size=\"11.00\" fill=\"#000000\">StandardScaler</text>\n",
       "</g>\n",
       "<!-- (&#39;clouds&#39;, &#39;humidity&#39;, &#39;pressure&#39;, &#39;temperature&#39;, &#39;wind&#39;)&#45;&gt;StandardScaler -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>(&#39;clouds&#39;, &#39;humidity&#39;, &#39;pressure&#39;, &#39;temperature&#39;, &#39;wind&#39;)&#45;&gt;StandardScaler</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-width=\".6\" d=\"M274.25,-215.8314C274.25,-215.8314 274.25,-190.4133 274.25,-190.4133\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" stroke-width=\".6\" points=\"277.7501,-190.4132 274.25,-180.4133 270.7501,-190.4133 277.7501,-190.4132\"/>\n",
       "</g>\n",
       "<!-- target_mean_by_station_and_hour -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>target_mean_by_station_and_hour</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" stroke-width=\"1.2\" points=\"522,-252 323,-252 323,-216 522,-216 522,-252\"/>\n",
       "<text text-anchor=\"middle\" x=\"422.5\" y=\"-231.2\" font-family=\"trebuchet\" font-size=\"11.00\" fill=\"#000000\">target_mean_by_station_and_hour</text>\n",
       "</g>\n",
       "<!-- get_hour&#45;&gt;target_mean_by_station_and_hour -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>get_hour&#45;&gt;target_mean_by_station_and_hour</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-width=\".6\" d=\"M368.5,-287.8314C368.5,-287.8314 368.5,-262.4133 368.5,-262.4133\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" stroke-width=\".6\" points=\"372.0001,-262.4132 368.5,-252.4133 365.0001,-262.4133 372.0001,-262.4132\"/>\n",
       "</g>\n",
       "<!-- target_mean_by_station_and_hour&#45;&gt;StandardScaler -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>target_mean_by_station_and_hour&#45;&gt;StandardScaler</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-width=\".6\" d=\"M327.25,-215.8314C327.25,-215.8314 327.25,-190.4133 327.25,-190.4133\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" stroke-width=\".6\" points=\"330.7501,-190.4132 327.25,-180.4133 323.7501,-190.4133 330.7501,-190.4132\"/>\n",
       "</g>\n",
       "<!-- LinearRegression -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>LinearRegression</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" stroke-width=\"1.2\" points=\"336,-108 239,-108 239,-72 336,-72 336,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"287.5\" y=\"-87.2\" font-family=\"trebuchet\" font-size=\"11.00\" fill=\"#000000\">LinearRegression</text>\n",
       "</g>\n",
       "<!-- StandardScaler&#45;&gt;LinearRegression -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>StandardScaler&#45;&gt;LinearRegression</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-width=\".6\" d=\"M287.5,-143.8314C287.5,-143.8314 287.5,-118.4133 287.5,-118.4133\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" stroke-width=\".6\" points=\"291.0001,-118.4132 287.5,-108.4133 284.0001,-118.4133 291.0001,-118.4132\"/>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>y</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" stroke-width=\"1.2\" points=\"314.5,-36 260.5,-36 260.5,0 314.5,0 314.5,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"287.5\" y=\"-15.2\" font-family=\"trebuchet\" font-size=\"11.00\" fill=\"#000000\">y</text>\n",
       "</g>\n",
       "<!-- LinearRegression&#45;&gt;y -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>LinearRegression&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-width=\".6\" d=\"M287.5,-71.8314C287.5,-71.8314 287.5,-46.4133 287.5,-46.4133\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" stroke-width=\".6\" points=\"291.0001,-46.4132 287.5,-36.4133 284.0001,-46.4133 291.0001,-46.4132\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f181531cbe0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `debug_one` method to see what happens to one particular instance. Let's train the model on the first 10,000 observations and then call `debug_one` on the next one. To do this, we will turn the `Bike` object into a Python generator with `iter()` function. The Pythonic way to read the first 10,000 elements of a generator is to use `itertools.islice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Input\n",
      "--------\n",
      "clouds: 0 (int)\n",
      "description: clear sky (str)\n",
      "humidity: 52 (int)\n",
      "moment: 2016-04-10 19:03:27 (datetime)\n",
      "pressure: 1,001.00000 (float)\n",
      "station: place-esquirol (str)\n",
      "temperature: 19.00000 (float)\n",
      "wind: 7.70000 (float)\n",
      "\n",
      "1. Transformer union\n",
      "--------------------\n",
      "    1.0 ('clouds', 'humidity', 'pressure', 'temperature', 'wind')\n",
      "    -------------------------------------------------------------\n",
      "    clouds: 0 (int)\n",
      "    humidity: 52 (int)\n",
      "    pressure: 1,001.00000 (float)\n",
      "    temperature: 19.00000 (float)\n",
      "    wind: 7.70000 (float)\n",
      "\n",
      "    1.1 get_hour | target_mean_by_station_and_hour\n",
      "    ----------------------------------------------\n",
      "    target_mean_by_station_and_hour: 7.97175 (float)\n",
      "\n",
      "clouds: 0 (int)\n",
      "humidity: 52 (int)\n",
      "pressure: 1,001.00000 (float)\n",
      "target_mean_by_station_and_hour: 7.97175 (float)\n",
      "temperature: 19.00000 (float)\n",
      "wind: 7.70000 (float)\n",
      "\n",
      "2. StandardScaler\n",
      "-----------------\n",
      "clouds: -1.36131 (float)\n",
      "humidity: -1.73074 (float)\n",
      "pressure: -1.26070 (float)\n",
      "target_mean_by_station_and_hour: 0.05495 (float)\n",
      "temperature: 1.76223 (float)\n",
      "wind: 1.45834 (float)\n",
      "\n",
      "3. LinearRegression\n",
      "-------------------\n",
      "Name                              Value      Weight     Contribution  \n",
      "                      Intercept    1.00000    7.43988        7.43988  \n",
      "                    temperature    1.76223    0.43250        0.76216  \n",
      "target_mean_by_station_and_hour    0.05495    2.30036        0.12641  \n",
      "                       humidity   -1.73074   -0.04168        0.07214  \n",
      "                         clouds   -1.36131    0.19230       -0.26177  \n",
      "                           wind    1.45834   -0.44409       -0.64763  \n",
      "                       pressure   -1.26070    1.65184       -2.08246  \n",
      "\n",
      "Prediction: 5.40872\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "X_y = iter(datasets.Bikes())\n",
    "\n",
    "model = compose.Whitelister('clouds', 'humidity', 'pressure', 'temperature', 'wind')\n",
    "model += (\n",
    "    get_hour |\n",
    "    feature_extraction.TargetAgg(by=['station', 'hour'], how=stats.Mean())\n",
    ")\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "for x, y in itertools.islice(X_y, 10000):\n",
    "    y_pred = model.predict_one(x)\n",
    "    model.fit_one(x, y)\n",
    "    \n",
    "x, y = next(X_y)\n",
    "model.debug_one(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `debug_one` method shows what happens to an input set of features, step by step.\n",
    "\n",
    "And now comes the catch. Up until now we've been using the `online_score` method from the `model_selection` module. What this does it that it sequentially predicts the output of an observation and updates the model immediately afterwards. This way of doing is often used for evaluating online learning models, but in some cases it is the wrong approach. \n",
    "\n",
    "The following paragraph is extremely important. When evaluating a machine learning model, the goal is to simulate production conditions in order to get a trust-worthy assessment of the performance of the model. In our case, we typically want to forecast the number of bikes available in a station, say, 30 minutes ahead. Then, once the 30 minutes have passed, the true number of available bikes will be available and we will be able to update the model using the features available 30 minutes ago. If you think about, this is exactly how a real-time machine learning system should work. The problem is that this isn't at all what the `online_score` method, indeed it is simply asking the model to predict the next observation, which is only a few minutes ahead, and then updates the model immediately. We can prove that this is flawed by adding a feature that measures a running average of the very recent values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 0."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_y = datasets.Bikes()\n",
    "\n",
    "model = compose.Whitelister('clouds', 'humidity', 'pressure', 'temperature', 'wind')\n",
    "model += (\n",
    "    get_hour |\n",
    "    feature_extraction.TargetAgg(by=['station', 'hour'], how=stats.Mean()) + \n",
    "    feature_extraction.TargetAgg(by='station', how=stats.EWMean(0.5))\n",
    ")\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "metric = metrics.MAE()\n",
    "\n",
    "model_selection.progressive_val_score(X_y, model, metric, print_every=20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score we got is too good to be true. This is simply because the problem is too easy. What we really want is to evaluate the model by forecasting 30 minutes ahead and only updating the model once the true values are available. This can be done using the `online_qa_score` method, also from the `model_selection` module. The \"qa\" part stands for \"question/answer\". The idea is that each observation of the stream of the data is shown twice to the model: once for making a prediction, and once for updating the model when the true value is revealed. The `on` parameter determines which variable should be used as a timestamp, while the `lag` parameter controls the duration to wait before revealing the true values to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 0."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "model = compose.Whitelister('clouds', 'humidity', 'pressure', 'temperature', 'wind')\n",
    "model += (\n",
    "    get_hour |\n",
    "    feature_extraction.TargetAgg(by=['station', 'hour'], how=stats.Mean()) + \n",
    "    feature_extraction.TargetAgg(by='station', how=stats.EWMean(0.5))\n",
    ")\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "model_selection.progressive_val_score(\n",
    "    X_y=datasets.Bikes(),\n",
    "    model=model,\n",
    "    metric=metrics.MAE(),\n",
    "    moment='moment',\n",
    "    delay=dt.timedelta(minutes=30),\n",
    "    print_every=20_000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score we now have is much more realistic, as it is comparable with [related data science competitions](https://maxhalford.github.io/blog/a-short-introduction-and-conclusion-to-the-openbikes-2016-challenge/). Moreover, we can see that the model gets better with time, which feels better than the previous situations. The point is that `online_qa_score` method can be used to simulate a production scenario, and is thus extremely valuable.\n",
    "\n",
    "Now that we have a working pipeline in place, we can attempt to make it more accurate. As a simple example, we'll using a `HedgeRegressor` from the `ensemble` module to combine 3 linear regression model trained with different optimizers. The `HedgeRegressor` will run the 3 models in parallel and assign weights to each model based on their individual performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<function progressive_val_score.<locals>.<lambda> at 0x7ff7070e58c8>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3962201cafd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmoment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'moment'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdelay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminutes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20_000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;32m~/Documents/creme-ml/creme/model_selection/score.py\u001b[0m in \u001b[0;36mprogressive_val_score\u001b[0;34m(X_y, model, metric, moment, delay, print_every, show_time, show_memory)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmoment_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/creme-ml/creme/model_selection/score.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(_, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mmoment_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mmoment_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mmoment_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmoment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: <function progressive_val_score.<locals>.<lambda> at 0x7ff7070e58c8>"
     ]
    }
   ],
   "source": [
    "from creme import ensemble\n",
    "from creme import optim\n",
    "\n",
    "model = compose.Whitelister('clouds', 'humidity', 'pressure', 'temperature', 'wind')\n",
    "model += (\n",
    "    get_hour |\n",
    "    feature_extraction.TargetAgg(by=['station', 'hour'], how=stats.Mean())\n",
    ")\n",
    "model += feature_extraction.TargetAgg(by='station', how=stats.EWMean(0.5))\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= ensemble.HedgeRegressor([\n",
    "    linear_model.LinearRegression(optim.SGD()),\n",
    "    linear_model.LinearRegression(optim.RMSProp()),\n",
    "    linear_model.LinearRegression(optim.Adam())\n",
    "])\n",
    "\n",
    "model_selection.progressive_val_score(\n",
    "    X_y=datasets.Bikes(),\n",
    "    model=model,\n",
    "    metric=metrics.MAE(),\n",
    "    moment='moment',\n",
    "    delay=dt.timedelta(minutes=30),\n",
    "    print_every=20_000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
