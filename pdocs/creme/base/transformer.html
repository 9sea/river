<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.5" />
<title>creme.base.transformer API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>creme.base.transformer</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import abc

from . import estimator


class Transformer(estimator.Estimator):
    &#34;&#34;&#34;A transformer.&#34;&#34;&#34;

    def fit_one(self, x: dict, y=None) -&gt; &#39;Transformer&#39;:
        &#34;&#34;&#34;Fits to a set of features ``x`` and an optional target ``y``.

        A lot of transformers don&#39;t actually have to do anything during the ``fit_one`` step
        because they are stateless. For this reason the default behavior of this function is to do
        nothing. Transformers that however do something during the ``fit_one`` can override this
        method.

        Parameters:
            x (dict)
            y (optional)

        Returns:
            self

        &#34;&#34;&#34;
        return self

    @abc.abstractmethod
    def transform_one(self, x: dict) -&gt; dict:
        &#34;&#34;&#34;Transforms a set of features ``x``.

        Parameters:
            x (dict)

        Returns:
            dict

        &#34;&#34;&#34;

    @property
    def is_supervised(self) -&gt; bool:
        &#34;&#34;&#34;Indicates if the transformer uses the target ``y`` or not.

        Supervised transformers have to be handled differently from unsupervised transformers in an
        online setting. This is especially true for target encoding where leakage can easily occur.
        Most transformers are unsupervised and so this property returns by default ``False``.
        Transformers that are supervised must override this property in their definition.

        &#34;&#34;&#34;
        return False

    def __or__(self, other):
        &#34;&#34;&#34;Merges with another Transformer into a Pipeline.&#34;&#34;&#34;
        from .. import compose
        if isinstance(other, compose.Pipeline):
            return other.__ror__(self)
        return compose.Pipeline(self, other)

    def __ror__(self, other):
        &#34;&#34;&#34;Merges with another Transformer into a Pipeline.&#34;&#34;&#34;
        from .. import compose
        if isinstance(other, compose.Pipeline):
            return other.__or__(self)
        return compose.Pipeline(other, self)

    def __add__(self, other):
        &#34;&#34;&#34;Merges with another Transformer into a TransformerUnion.&#34;&#34;&#34;
        from .. import compose
        if isinstance(other, compose.TransformerUnion):
            return other.__add__(self)
        return compose.TransformerUnion(self, other)

    def __radd__(self, other):
        &#34;&#34;&#34;Merges with another Transformer into a TransformerUnion.&#34;&#34;&#34;
        from .. import compose
        if isinstance(other, compose.TransformerUnion):
            return other.__add__(self)
        return compose.TransformerUnion(other, self)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="creme.base.transformer.Transformer"><code class="flex name class">
<span>class <span class="ident">Transformer</span></span>
</code></dt>
<dd>
<section class="desc"><p>A transformer.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Transformer(estimator.Estimator):
    &#34;&#34;&#34;A transformer.&#34;&#34;&#34;

    def fit_one(self, x: dict, y=None) -&gt; &#39;Transformer&#39;:
        &#34;&#34;&#34;Fits to a set of features ``x`` and an optional target ``y``.

        A lot of transformers don&#39;t actually have to do anything during the ``fit_one`` step
        because they are stateless. For this reason the default behavior of this function is to do
        nothing. Transformers that however do something during the ``fit_one`` can override this
        method.

        Parameters:
            x (dict)
            y (optional)

        Returns:
            self

        &#34;&#34;&#34;
        return self

    @abc.abstractmethod
    def transform_one(self, x: dict) -&gt; dict:
        &#34;&#34;&#34;Transforms a set of features ``x``.

        Parameters:
            x (dict)

        Returns:
            dict

        &#34;&#34;&#34;

    @property
    def is_supervised(self) -&gt; bool:
        &#34;&#34;&#34;Indicates if the transformer uses the target ``y`` or not.

        Supervised transformers have to be handled differently from unsupervised transformers in an
        online setting. This is especially true for target encoding where leakage can easily occur.
        Most transformers are unsupervised and so this property returns by default ``False``.
        Transformers that are supervised must override this property in their definition.

        &#34;&#34;&#34;
        return False

    def __or__(self, other):
        &#34;&#34;&#34;Merges with another Transformer into a Pipeline.&#34;&#34;&#34;
        from .. import compose
        if isinstance(other, compose.Pipeline):
            return other.__ror__(self)
        return compose.Pipeline(self, other)

    def __ror__(self, other):
        &#34;&#34;&#34;Merges with another Transformer into a Pipeline.&#34;&#34;&#34;
        from .. import compose
        if isinstance(other, compose.Pipeline):
            return other.__or__(self)
        return compose.Pipeline(other, self)

    def __add__(self, other):
        &#34;&#34;&#34;Merges with another Transformer into a TransformerUnion.&#34;&#34;&#34;
        from .. import compose
        if isinstance(other, compose.TransformerUnion):
            return other.__add__(self)
        return compose.TransformerUnion(self, other)

    def __radd__(self, other):
        &#34;&#34;&#34;Merges with another Transformer into a TransformerUnion.&#34;&#34;&#34;
        from .. import compose
        if isinstance(other, compose.TransformerUnion):
            return other.__add__(self)
        return compose.TransformerUnion(other, self)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="creme.base.estimator.Estimator" href="estimator.html#creme.base.estimator.Estimator">Estimator</a></li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li>creme.compose.func.FuncTransformer</li>
<li>creme.compose.rename.Renamer</li>
<li>creme.compose.select.Discard</li>
<li>creme.compose.select.Select</li>
<li>creme.compose.union.TransformerUnion</li>
<li>creme.decomposition.lda.LDA</li>
<li>creme.feature_extraction.agg.Agg</li>
<li>creme.feature_extraction.agg.TargetAgg</li>
<li>creme.feature_extraction.differ.Differ</li>
<li>creme.feature_extraction.vectorize.BagOfWords</li>
<li>creme.feature_selection.k_best.SelectKBest</li>
<li>creme.feature_selection.random.PoissonInclusion</li>
<li>creme.feature_selection.variance.VarianceThreshold</li>
<li>creme.impute.previous.PreviousImputer</li>
<li>creme.impute.stat.StatImputer</li>
<li>creme.preprocessing.feature_hasher.FeatureHasher</li>
<li>creme.preprocessing.kernel_approx.RBFSampler</li>
<li>creme.preprocessing.one_hot.OneHotEncoder</li>
<li>creme.preprocessing.poly.PolynomialExtender</li>
<li>creme.preprocessing.scale.Binarizer</li>
<li>creme.preprocessing.scale.MaxAbsScaler</li>
<li>creme.preprocessing.scale.MinMaxScaler</li>
<li>creme.preprocessing.scale.Normalizer</li>
<li>creme.preprocessing.scale.RobustScaler</li>
<li>creme.preprocessing.scale.StandardScaler</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="creme.base.transformer.Transformer.is_supervised"><code class="name">var <span class="ident">is_supervised</span></code></dt>
<dd>
<section class="desc"><p>Indicates if the transformer uses the target <code>y</code> or not.</p>
<p>Supervised transformers have to be handled differently from unsupervised transformers in an
online setting. This is especially true for target encoding where leakage can easily occur.
Most transformers are unsupervised and so this property returns by default <code>False</code>.
Transformers that are supervised must override this property in their definition.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_supervised(self) -&gt; bool:
    &#34;&#34;&#34;Indicates if the transformer uses the target ``y`` or not.

    Supervised transformers have to be handled differently from unsupervised transformers in an
    online setting. This is especially true for target encoding where leakage can easily occur.
    Most transformers are unsupervised and so this property returns by default ``False``.
    Transformers that are supervised must override this property in their definition.

    &#34;&#34;&#34;
    return False</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="creme.base.transformer.Transformer.fit_one"><code class="name flex">
<span>def <span class="ident">fit_one</span></span>(<span>self, x: dict, y=None) -> <a title="creme.base.transformer.Transformer" href="#creme.base.transformer.Transformer">Transformer</a></span>
</code></dt>
<dd>
<section class="desc"><p>Fits to a set of features <code>x</code> and an optional target <code>y</code>.</p>
<p>A lot of transformers don't actually have to do anything during the <code>fit_one</code> step
because they are stateless. For this reason the default behavior of this function is to do
nothing. Transformers that however do something during the <code>fit_one</code> can override this
method.</p>
<h2 id="parameters">Parameters</h2>
<p>x (dict)
y (optional)</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>self</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_one(self, x: dict, y=None) -&gt; &#39;Transformer&#39;:
    &#34;&#34;&#34;Fits to a set of features ``x`` and an optional target ``y``.

    A lot of transformers don&#39;t actually have to do anything during the ``fit_one`` step
    because they are stateless. For this reason the default behavior of this function is to do
    nothing. Transformers that however do something during the ``fit_one`` can override this
    method.

    Parameters:
        x (dict)
        y (optional)

    Returns:
        self

    &#34;&#34;&#34;
    return self</code></pre>
</details>
</dd>
<dt id="creme.base.transformer.Transformer.transform_one"><code class="name flex">
<span>def <span class="ident">transform_one</span></span>(<span>self, x: dict) -> dict</span>
</code></dt>
<dd>
<section class="desc"><p>Transforms a set of features <code>x</code>.</p>
<h2 id="parameters">Parameters</h2>
<p>x (dict)</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abc.abstractmethod
def transform_one(self, x: dict) -&gt; dict:
    &#34;&#34;&#34;Transforms a set of features ``x``.

    Parameters:
        x (dict)

    Returns:
        dict

    &#34;&#34;&#34;</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="creme.base" href="index.html">creme.base</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="creme.base.transformer.Transformer" href="#creme.base.transformer.Transformer">Transformer</a></code></h4>
<ul class="">
<li><code><a title="creme.base.transformer.Transformer.fit_one" href="#creme.base.transformer.Transformer.fit_one">fit_one</a></code></li>
<li><code><a title="creme.base.transformer.Transformer.is_supervised" href="#creme.base.transformer.Transformer.is_supervised">is_supervised</a></code></li>
<li><code><a title="creme.base.transformer.Transformer.transform_one" href="#creme.base.transformer.Transformer.transform_one">transform_one</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.5</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>